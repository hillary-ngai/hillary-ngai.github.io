---
layout: post
title: Understanding Variational Autoencoders
---

## What are Variational Autoencoders?
![VAE Diagram](../images/VAE.png)Diagram of a Variational Autoencoder.

Variational Autoencoders (VAEs) are generative models with an encoder-decoder architecture.
Just like a standard autoencoder, VAEs are trained in an unsupervised manner 
where the reconstruction error between the input x and the 
reconstructed input x' is minimized. However, in order to introduce some regularization of the latent space, 
the input is encoded as a distribution over the latent space rather than encoded as a single point. In other words, 
z is now represented as a distribution, rather than a vector.


VAEs encode the input x to the latent representation z and reconstruct the input by
decoding z to x'.

VAEs make strong assumptions about the distribution of the latent variables.
* they use a variational approach for latent representation learning
* assumes that the data is being generated by a directed graphical model p(x|z)
and that the encoder is learning an approximation q(z|x) to the posterior distribution p(z|x)
encoder = recognition model
decoder = generative model
* the prior is usually set to be the centred isotropic multivariate guassian p(z) = N(0, I)
* the shape of the variational and the likelihood distributions are also gaussians
Let's dive into the math behind Variational Autoencoders (VAEs)!