---
layout: post
title: Understanding Variational Autoencoders
---

## Introduction
Deep generative models have shown an incredible results in 
producing highly realistic pieces of content of various kind, 
such as images, texts, and music. The three most popular
generative model approaches are Generative Adversarial Networks (GANs), autoregressive models, 
and Variational Autoencoders (VAEs). However, this blogpost will only be focusing on VAEs.

### What are Variational Autoencoders?
![VAE Diagram](../images/VAE.png)Diagram of a Variational Autoencoder.

Variational Autoencoders are generative models with an encoder-decoder architecture.
Just like a standard autoencoder, VAEs are trained in an unsupervised manner 
where the reconstruction error between the input x and the 
reconstructed input x' is minimized. However, in order to introduce some regularization of the latent space, 
the input is encoded as a distribution over the latent space rather than encoded as a single point. In other words, 
VAEs represent z as a distribution, rather than a vector.

### Generating New Images
To generate a random, new sample that is similar to the training data, you can simply remove the encoder
portion of the VAE and randomly sample from the latent distribution. 
This will give you a latent vector z which can then be decoded using the decoder to 
produce the new sample x'.

## Math Derivation
Now that you have some intuition of how VAEs work, let's dive into the
maths behind VAEs!

### Kullback–Leibler Divergence
Let's first define the Kullback–Leibler (KL) divergence (also called relative entropy).

For discrete probability distributions P and Q defined on the same probability space, X
the relative entropy from Q to P is defined to be:
![KL Divergence](../images/KL-divergence.svg)Diagram of a Variational Autoencoder.





### Etc.
VAEs make strong assumptions about the distribution of the latent variables.

* assumes that the data is being generated by a directed graphical model p(x|z)
and that the encoder is learning an approximation q(z|x) to the posterior distribution p(z|x)
* the prior is usually set to be the centred isotropic multivariate guassian p(z) = N(0, I)
* the shape of the variational and the likelihood distributions are also gaussians