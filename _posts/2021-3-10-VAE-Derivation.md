---
layout: post
title: Understanding Variational Autoencoders
---

![VAE Diagram](/Users/hillary/Documents/hillary-ngai.github.io/images/VAE.png)

Variational Autoencoders (VAEs) are generative models with an encoder-decoder architecture.
* VAEs are directed probabilistic graphical models (DPGM) whose posterior is 
approximated using a neural network.
* a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.
* Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space.
* generative modeling tries to learn how the data is generated, and to reflect the underlying causal relations
make strong assumptions about the distribution of the latent variables.
* they use a variational approach for latent representation learning
* assumes that the data is being generated by a directed graphical model p(x|z)
and that the encoder is learning an approximation q(z|x) to the posterior distribution p(z|x)
encoder = recognition model
decoder = generative model
* the prior is usually set to be the centred isotropic multivariate guassian p(z) = N(0, I)
* the shape of the variational and the likelihood distributions are also gaussians
Let's dive into the math behind Variational Autoencoders (VAEs)!